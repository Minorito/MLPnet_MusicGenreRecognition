# -*- coding: utf-8 -*-
"""RedeKD3_OK.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1S526g21APGWO5rmVy6y-4Fd3jrRjHW39
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style('whitegrid')
# %matplotlib inline
import warnings
warnings.filterwarnings('ignore')
import tensorflow as tf
import keras as k
import sklearn.metrics as skm
import sklearn.model_selection as skms
import sklearn.preprocessing as skp
import random
import librosa, IPython
import librosa.display as lplt
from sklearn.metrics import accuracy_score, plot_confusion_matrix, classification_report
import seaborn as sn
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import KFold
from tensorflow.keras.losses import sparse_categorical_crossentropy
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.optimizers import RMSprop

seed = 10
np.random.seed(seed)
import numpy as np
import matplotlib.pyplot as plt
from itertools import cycle

from sklearn import svm, datasets
from sklearn.metrics import roc_curve, auc
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import label_binarize
from sklearn.multiclass import OneVsRestClassifier
from sklearn.metrics import roc_auc_score

from google.colab import drive
drive.mount('/content/drive/')

data = pd.read_csv('/content/drive/My Drive/musicas/datasets/Data/features_3_sec.csv')

label_index = dict()
index_label = dict()
for i, x in enumerate(data.label.unique()):
    label_index[x] = i
    index_label[i] = x
print(label_index)
print(index_label)

data.label = [label_index[l] for l in data.label]

data_shuffle = data.sample(frac=1).reset_index(drop=True)

data_shuffle.drop(['filename', 'length'], axis=1, inplace=True)
data_y = data_shuffle.pop('label')
data_x = data_shuffle

x_train,x_test,y_train,y_test=skms.train_test_split(data_x,data_y,test_size=0.3,random_state=42)

scaler = skp.StandardScaler()
x_train = pd.DataFrame(scaler.fit_transform(x_train), columns=x_train.columns)
x_test = pd.DataFrame(scaler.transform(x_test), columns=x_train.columns)

acc_per_fold = []
loss_per_fold = []

inputs = np.concatenate((x_train, x_test), axis=0)
targets = np.concatenate((y_train, y_test), axis=0)

batch_size = 100 #Quanto colocar batch-size
loss_function = sparse_categorical_crossentropy
no_epochs = 100
optimizer = Adam()
verbosity = 0
num_folds = 10 #
fold_no = 1

kfold = KFold(n_splits=num_folds, shuffle=True)

for train, test in kfold.split(inputs, targets):
  callback = tf.keras.callbacks.EarlyStopping(monitor='loss', mode='min', verbose=1, patience=5) #Função para parar antes
  # Definindo Arquitetura
  model = k.models.Sequential([
      k.layers.Dense(57, activation='relu', input_shape=(x_train.shape[1],)), #Camada entrada é igual quantidade de características
      k.layers.Dropout(0.1),

      k.layers.Dense(342, activation='relu'), #Camada de entrada vezes 6
      k.layers.Dropout(0.1),

      k.layers.Dense(171, activation='relu'), #Camada anterior dividido por 2
      k.layers.Dropout(0.1),

      k.layers.Dense(10, activation='softmax'), #Camada de saída são os 10 gêneros
  ])

  model.compile(optimizer=optimizer, loss=loss_function, metrics='accuracy')

  print('------------------------------------------------------------------------')
  print(f'Training for fold {fold_no} ...')

  history = model.fit(inputs[train], targets[train],
              batch_size=batch_size,
              epochs=no_epochs,callbacks=[callback],
              verbose=verbosity)

  scores = model.evaluate(inputs[test], targets[test], verbose=0)
  print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')
  acc_per_fold.append(scores[1] * 100)
  loss_per_fold.append(scores[0])

  fold_no = fold_no + 1

# == Provide average scores ==
print('------------------------------------------------------------------------')
print('Score per fold')
for i in range(0, len(acc_per_fold)):
  print('------------------------------------------------------------------------')
  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')

print('------------------------------------------------------------------------')
print('Average scores for all folds:')
print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')
print(f'> Loss: {np.mean(loss_per_fold)}')
print('------------------------------------------------------------------------')

print("Max Accuracy",max(history.history["accuracy"]))
pd.DataFrame(history.history).plot(figsize=(12,6))
plt.show()

prediction = model.predict(inputs[test])
predicted_index = np.argmax(prediction, axis = 1)

cm = confusion_matrix(targets[test], predicted_index)

plt.figure(figsize = (20, 10))
sns.heatmap(cm, cmap="flare", annot=True, 
            xticklabels = ["blues", "classical", "country", "disco", "hiphop", "jazz", "metal", "pop", "reggae", "rock"],
            yticklabels=["blues", "classical", "country", "disco", "hiphop", "jazz", "metal", "pop", "reggae", "rock"]);

matriz_total= np.sum(cm)
print(f'Soma de todos elementos da matriz {matriz_total}')

n = 10

diagonal_principal = sum(cm[i][i] for i in range(n))
print(f'Soma diagonal principal {diagonal_principal}')

print(f'Matriz Confusão: {(diagonal_principal/matriz_total)*100}%')

test_loss, test_acc  = model.evaluate(inputs[test], targets[test])
print("The test Loss is :",test_loss)
print("\nThe Best test Accuracy is :",test_acc*100)

genres = ["blues", "classical", "country", "disco", "hiphop", "jazz", "metal", "pop", "reggae", "rock"]
print(classification_report(targets[test], predicted_index, target_names=genres))
